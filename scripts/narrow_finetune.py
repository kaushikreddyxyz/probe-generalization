# ABOUTME: CLI entrypoint to narrow finetuning on sycophancy data with optional LoRA and push-to-hub.
# ABOUTME: Delegates implementation to llm.finetune modules for reuse and tests.

import argparse
from typing import Iterable

from llm.finetune.config import build_config, MODEL_DEFAULT, DATASET_DEFAULT, EVAL_DATASET_DEFAULT, OUTPUT_DIR_DEFAULT, NUM_EPOCHS_DEFAULT, LR_DEFAULT, WEIGHT_DECAY_DEFAULT, BATCH_SIZE_DEFAULT, GRAD_ACCUM_DEFAULT, WARMUP_STEPS_DEFAULT, MAX_SEQ_LEN_DEFAULT, VAL_SPLIT_DEFAULT, LORA_R_DEFAULT, LORA_ALPHA_DEFAULT, LORA_DROPOUT_DEFAULT, TARGET_MODULES_DEFAULT, LOGGING_STEPS_DEFAULT, SAVE_STEPS_DEFAULT, SAVE_TOTAL_LIMIT_DEFAULT, SYSTEM_PROMPT_DEFAULT, WANDB_PROJECT, WANDB_ON_DEFAULT
from llm.finetune.runner import run_training


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Narrow finetune Gemma on sycophancy data with optional LoRA + hub push.")
    parser.add_argument("--model_name", type=str, default=MODEL_DEFAULT, help="HF model id or local path.")
    parser.add_argument("--dataset_path", type=str, default=DATASET_DEFAULT, help="Path to JSONL training data.")
    parser.add_argument(
        "--eval_dataset_path",
        type=str,
        default=EVAL_DATASET_DEFAULT,
        help="Optional JSONL eval set; if omitted and --val_split>0, a split is created from train.",
    )
    parser.add_argument("--output_dir", type=str, default=OUTPUT_DIR_DEFAULT, help="Where to store checkpoints.")
    parser.add_argument("--num_epochs", type=int, default=NUM_EPOCHS_DEFAULT, help="Number of training epochs.")
    parser.add_argument("--learning_rate", type=float, default=LR_DEFAULT, help="Optimizer learning rate.")
    parser.add_argument("--weight_decay", type=float, default=WEIGHT_DECAY_DEFAULT, help="Weight decay.")
    parser.add_argument("--batch_size", type=int, default=BATCH_SIZE_DEFAULT, help="Per-device batch size.")
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=GRAD_ACCUM_DEFAULT,
        help="Gradient accumulation steps.",
    )
    parser.add_argument("--warmup_steps", type=int, default=WARMUP_STEPS_DEFAULT, help="Warmup steps.")
    parser.add_argument(
        "--max_steps",
        type=int,
        default=-1,
        help="Override total training steps. Leave -1 to use num_epochs.",
    )
    parser.add_argument(
        "--max_seq_length",
        type=int,
        default=MAX_SEQ_LEN_DEFAULT,
        help="Sequence length after chat templating.",
    )
    parser.add_argument(
        "--val_split",
        type=float,
        default=VAL_SPLIT_DEFAULT,
        help="Holdout fraction from training file when no explicit eval file is provided.",
    )
    parser.add_argument(
        "--use_lora",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Enable LoRA adapter finetuning instead of full-model updates.",
    )
    parser.add_argument("--lora_r", type=int, default=LORA_R_DEFAULT, help="LoRA rank.")
    parser.add_argument("--lora_alpha", type=int, default=LORA_ALPHA_DEFAULT, help="LoRA alpha scaling.")
    parser.add_argument("--lora_dropout", type=float, default=LORA_DROPOUT_DEFAULT, help="LoRA dropout.")
    parser.add_argument(
        "--target_modules",
        nargs="+",
        default=TARGET_MODULES_DEFAULT,
        help="Module name fragments to receive LoRA adapters.",
    )
    parser.add_argument(
        "--override_system_prompt",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Replace dataset system prompts with --system_prompt.",
    )
    parser.add_argument(
        "--system_prompt",
        type=str,
        default=SYSTEM_PROMPT_DEFAULT,
        help="System prompt used only when --override_system_prompt is true.",
    )
    parser.add_argument("--bf16", action=argparse.BooleanOptionalAction, default=True, help="Use bfloat16 if available.")
    parser.add_argument("--logging_steps", type=int, default=LOGGING_STEPS_DEFAULT, help="Trainer logging steps.")
    parser.add_argument("--save_steps", type=int, default=SAVE_STEPS_DEFAULT, help="Checkpoint save frequency.")
    parser.add_argument(
        "--save_total_limit",
        type=int,
        default=SAVE_TOTAL_LIMIT_DEFAULT,
        help="Max checkpoints to keep before rotating.",
    )
    parser.add_argument("--wandb_on", action=argparse.BooleanOptionalAction, default=WANDB_ON_DEFAULT, help="Enable wandb logging.")
    parser.add_argument("--wandb_project", type=str, default=WANDB_PROJECT, help="wandb project name.")
    parser.add_argument("--wandb_entity", type=str, default=None, help="wandb entity (optional).")
    parser.add_argument("--run_name", type=str, default=None, help="wandb/Trainer run name; autogenerated if omitted.")
    parser.add_argument(
        "--push_to_hub",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="If true, push the checkpoint to the specified hub repo.",
    )
    parser.add_argument(
        "--hub_model_id",
        type=str,
        default=None,
        help="Repository name on the Hugging Face Hub for push_to_hub.",
    )
    return parser


def main(argv: Iterable[str] | None = None):
    parser = build_arg_parser()
    args = parser.parse_args(list(argv) if argv is not None else None)
    cfg = build_config(args)
    run_training(cfg)


if __name__ == "__main__":
    main()
